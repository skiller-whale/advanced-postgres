#! /usr/bin/env python3

import hashlib
import json
import os
import re
import threading
import time
from urllib.parse import urljoin

import requests
import psycopg2
import tabulate

import db_init


WATCHED_EXTS = [".sh", ".sql"]
IGNORE_DIRS = [".git"]

SERVER_URL = os.getenv('SERVER_URL', "https://train.skillerwhale.com")
USER_ID_FILEPATH = '/attendance_id'

OUTPUT_LOCK = threading.Lock()


# This is useful for, e.g., displaying EXPLAIN output where indentation should be preserved.
tabulate.PRESERVE_WHITESPACE = True


def run_sync():
    file_uploader = FileUploader()
    pinger = Pinger()
    watcher = Watcher(file_uploader, base_path="src")
    watcher.poll_for_changes(loop_callback=pinger.ping)


def pg_update():
    time.sleep(1)  # Give time for the database service to start
    pg_executor = PgExec()
    watcher = Watcher(pg_executor, base_path="src")
    watcher.poll_for_changes()


def read_attendance_id():
    with open(USER_ID_FILEPATH) as id_file:
        return id_file.read().strip()


def skiller_whale_sync():
    print("  _____ _    _ _ _            __          ___           _      ")
    print(" / ____| |  (_) | |           \\ \\        / / |         | |     ")
    print("| (___ | | ___| | | ___ _ __   \\ \\  /\\  / /| |__   __ _| | ___ ")
    print(" \\___ \\| |/ / | | |/ _ \\ '__|   \\ \\/  \\/ / | '_ \\ / _` | |/ _ \\")
    print(" ____) |   <| | | |  __/ |       \\  /\\  /  | | | | (_| | |  __/")
    print("|_____/|_|\\_\\_|_|_|\\___|_|        \\/  \\/   |_| |_|\\__,_|_|\\___| ")
    print("")
    print("")
    print(f'We\'re going to start watching this directory for changes so that the trainer can see your progress.')
    print(f'Your attendance id is currently set to: {read_attendance_id()}')
    print(f'If this is not correct, you can update it in the file {USER_ID_FILEPATH}\n')

    sync_thread = threading.Thread(target=run_sync)
    sync_thread.start()

    pg_update()


class PgExec:
    DATABASE_NAME_FILE = '.db_name'
    # Matches any integer, surrounded by '.' just before the file extension
    # (e.g. the 11 in my_file.11.sql) using lookbehind and lookahead groups
    # Note: Because of the $ at the end, this can match at most one group.
    FILE_NUMBER_REGEX = re.compile(r'(?<=\.)\d+(?=\.[^\.]+$)')

    # This is an extra safety measure to ensure that db dumps, and restores
    # never clash with each other, or with user queries.
    DATABASE_LOCK = threading.Lock()

    def __init__(self):
        print("Initialising databases")
        db_init.init()
        print("Initialisation complete")

    def execute_sql(self, connection, sql):
        with connection.cursor() as cursor:
            cursor.execute(sql)

            if not cursor.description:  # If no columns are returned
                return [], []

            column_names = [column.name for column in cursor.description]
            return column_names, cursor.fetchall()

    @classmethod
    def get_db_name_from_path(cls, path):
        """Returns the content of a .db_name file in the directory"""
        dir_name, _ = os.path.split(path)
        db_file = os.path.join(dir_name, cls.DATABASE_NAME_FILE)

        if os.path.exists(db_file):
            with open(db_file) as file:
                db_name = file.read().strip()
                if db_name:
                    return db_name

        print(f"\nNo associated database found at {db_file}\n")
        return None

    def run_query(self, path, database_name, identifier=None):
        # Only display an identifier if one has been passed into the functino
        identifier_str = f"<{identifier}> | " if identifier else ""

        with OUTPUT_LOCK:
            print(f'{identifier_str}Running File {path}')

        with open(path, "r") as f:
            contents = f.read()

        with OUTPUT_LOCK:
            print(f"{identifier_str}Querying the database: {database_name}")

        # Do not retry - an invalid db name would get stuck in an infinite loop
        connection = db_init.get_connection(database_name, retry_time=None)
        if not connection:
            return  # Database error logging dealt with in get_connection

        try:
            query_columns, query_rows = self.execute_sql(connection, contents)
        except psycopg2.Error as err:
            connection.rollback()
            with OUTPUT_LOCK:
                print(f'{identifier_str}POSTGRES ERROR:', err)
                print()
        else:
            with OUTPUT_LOCK:
                if identifier:
                    print(f'{identifier_str}Query Complete. Results:')
                print(tabulate.tabulate(query_rows,
                                        headers=query_columns,
                                        tablefmt="psql"))
                print(f'Row Count: {len(query_rows)}')
                print()

    @classmethod
    def _get_related_file_regex(cls, path):
        """Returns a regex for matching 'peer' files of path

        If path were file_name.1.sql, then this regex would match any file with
        the structure file_name.X.sql where X is any integer.

        If FILE_NUMBER_REGEX is not matched, then return None
        """
        if not cls.FILE_NUMBER_REGEX.search(path):
            return None

        file_components = map(re.escape, cls.FILE_NUMBER_REGEX.split(path))
        return re.compile(r'(\d+)'.join(file_components))

    @classmethod
    def _get_files_and_identifiers_to_run(cls, path):
        """Identifies which other files should be run, when path is edited"""
        related_regex = cls._get_related_file_regex(path)
        if not related_regex:
            return { path: None }

        dir_name, _ = os.path.split(path)
        matches = [
            related_regex.fullmatch(os.path.join(dir_name, file))
            for file in os.listdir(dir_name)
            if os.path.isfile(os.path.join(dir_name, file))  # Not directories
        ]
        # Return dictionary of { full_path : number_identifier }
        return { match.group(0): match.group(1) for match in matches if match}

    def run_threaded_queries(self, path, database_name):
        paths_to_run = self._get_files_and_identifiers_to_run(path)
        # Create one thread per related path to run.
        threads = [
            threading.Thread(
                target=self.run_query,
                args=(path, database_name, identifier)
            )
            for path, identifier in paths_to_run.items()
        ]
        for thread in threads:
            thread.start()
        # Ensure that all threads have completed before allowing the
        # main thread to start executing any new queries.
        for thread in threads:
            thread.join()

    def file_changed(self, path):
        # Will run a query against the database named according to the path
        database_name = self.get_db_name_from_path(path)
        if not database_name:
            return

        # Backup the state of the database before any queries have run, so it
        # can be restored afterwards.
        with self.DATABASE_LOCK:
            backup = db_init.pg_dump(database_name)

        try:
            with self.DATABASE_LOCK:
                self.run_threaded_queries(path, database_name)
        except Exception as err:
            with OUTPUT_LOCK:
                print("Unexpected error running query:", err)
        finally:
            with OUTPUT_LOCK:
                print(f"Rolling back changes to {database_name}...")

            # Restore the backup, to reset the database
            with self.DATABASE_LOCK:
                db_init.pg_restore(database_name, backup)

            with OUTPUT_LOCK:
                print("Done")
                print()


def create_skiller_whale_url(path):
    return urljoin(SERVER_URL, path)


class FileUploader:
    def get_uri(self, attendance_id):
        return create_skiller_whale_url(self.get_path(attendance_id))

    def get_path(self, attendance_id):
        return f'attendances/{attendance_id}/file_snapshots'

    @staticmethod
    def get_file_data(path):
        with open(path, "r") as f:
            data = {"relative_path": path, "contents": f.read()}
            return json.dumps(data)

    @staticmethod
    def get_headers(data):
        return {
            "Content-Type": "application/json",
            "Content-Length": str(len(data))
        }

    def post_file(self, path, attendance_id):
        data = self.get_file_data(path)
        headers = self.get_headers(data)
        return requests.post(self.get_uri(attendance_id), data=data, headers=headers)

    def file_changed(self, path):
        attendance_id = read_attendance_id()
        with OUTPUT_LOCK:
            print(f"Uploading: {path}", end='\t')
        if not attendance_id:
            with OUTPUT_LOCK:
                print("No attendance id set; file update not sent.")
            return

        try:
            response = self.post_file(path, attendance_id)
        except requests.exceptions.ConnectionError:
            with OUTPUT_LOCK:
                print(f"Failed\nCould not reach {SERVER_URL}")
        else:
            with OUTPUT_LOCK:
                print(f"Status: {response.status_code}")
            if response.text:
                with OUTPUT_LOCK:
                    print(response.text)


class Pinger:
    @property
    def uri(self):
        return create_skiller_whale_url(self.path)

    @property
    def path(self):
        attendance_id = read_attendance_id()
        return f'attendances/{attendance_id}/pings'

    def ping(self):
        try:
            requests.post(self.uri)
        except requests.exceptions.ConnectionError:
            pass  # Tolerate failed pings


class Watcher:
    def __init__(self, responder, base_path='.'):
        self.responder = responder
        self.base_path = base_path
        self._file_hashes = {}
        # Tracks whether this is the first pass of the directory tree. If not,
        # then any new file encountered will be treated as an update.
        self._first_pass = True

    @staticmethod
    def get_file_hash(path):
        """Return a hash digest of the file located at path"""
        with open(path, "rb") as f:
            contents = f.read()
            return hashlib.md5(contents).hexdigest()

    def _respond_to_file_change(self, path):
        _, extension = os.path.splitext(path)
        if extension not in WATCHED_EXTS:
            return

        hashed = self.get_file_hash(path)
        if not self._first_pass:
            old_hash = self._file_hashes.get(path)
            if old_hash != hashed:
                try:
                    self.responder.file_changed(path)
                except Exception as err:
                    with OUTPUT_LOCK:
                        print("Unexpected error:", err)
        self._file_hashes[path] = hashed

    def _check_dir_for_changes(self, dir_path):
        if os.path.basename(dir_path) in IGNORE_DIRS:
            return

        for filename in os.listdir(dir_path):
            new_path = os.path.join(dir_path, filename)
            if os.path.isdir(new_path):
                # Recursively check subdirectories
                self._check_dir_for_changes(new_path)
            else:
                self._respond_to_file_change(new_path)

    def poll_for_changes(self, wait_time=1, loop_callback=lambda: None):
        """Optionally specify loop_callback, which is called each iteration"""
        while True:
            try:
                loop_callback()
            except Exception as err:
                with OUTPUT_LOCK:
                    print("Unexpected callback error:", err)

            try:
                self._check_dir_for_changes(self.base_path)
            except Exception as err:
                with OUTPUT_LOCK:
                    print("Unexpected error in file watcher:", err)
            else:
                self._first_pass = False

            time.sleep(wait_time)  # Poll for changes every `wait_time` seconds


if __name__ == "__main__":
    skiller_whale_sync()
